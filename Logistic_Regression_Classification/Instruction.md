

# Part I: Logistic Regression

In this section, we use `fmin_tnc` package from `scipy` to find the optimal parameter theta instead of Gradient Descent  

We final get the optimal theta as:  

```
Optimal theta: [-25.16131863   0.20623159   0.20147149]
final cost function: 0.2034977015894746
We got the final accuracy is: 89.0
```

We also plot the decision boundary  

![](https://www.cnblogs.com/images/cnblogs_com/haochen273/1389248/o_01.png)  

## Part II: Logistic Regression with Regularization  

We can get the final result from `fmin_tnc`

```
Optimal theta is: [-25.07331893   0.20555364   0.200723  ]
Accuracy: 89.0
```

